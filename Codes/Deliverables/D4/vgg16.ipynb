{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "655d3b36",
   "metadata": {},
   "source": [
    "# Deliverable #4 - VGG16\n",
    "\n",
    "This notebook is dedicated solely to the **`VGG16`**'s fine tuning.\n",
    "\n",
    "See [index notebook](index.ipynb) for instructions and other list of derivative notebooks created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7383b395",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "All needed libraries will be imported here.\n",
    "\n",
    "Unless conditional, all imports must be done in this section to prevent workspace cluttering. Imports are sorted in an ascending manner, starting from \"a\" to \"Z\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b7f0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import backend as K, mixed_precision\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import BatchNormalization, Dense, Dropout, Flatten, Input, Resizing\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from typing import Union\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from helpers import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfec4b98",
   "metadata": {},
   "source": [
    "## Data and Variable\n",
    "\n",
    "Sets all the global data and variables here.\n",
    "\n",
    "Global variables will be defined and instantiated in this section, preventing a confusing clutter down the line and allowing readability when revisions are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a750ba75",
   "metadata": {},
   "source": [
    "### Instantiations\n",
    "\n",
    "Instantiations of variables will be done here, preventing mixture of variable preview and definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883ea9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = {\n",
    "    \"train\": {\n",
    "        \"raw\": [os.path.join('data', file) for file in os.listdir(\"data\") if file.startswith('data_batch_')],\n",
    "        \"loaded\": {},\n",
    "        \"processed\": None\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"raw\": os.path.join('data', 'test_batch'),\n",
    "        \"loaded\": {},\n",
    "        \"processed\": None\n",
    "    },\n",
    "    \"meta\": os.path.join('data', 'batches.meta')\n",
    "}\n",
    "\"\"\"\n",
    "A dictionary to hold the data for the CIFAR-10 dataset.\n",
    "The dictionary contains the following keys:\n",
    "\n",
    "- train: A dictionary containing the training data. It has two keys:\n",
    "    - raw: A list of file paths for the training data files.\n",
    "    - loaded: A dictionary to hold the loaded training data.\n",
    "    - processed: A dictionary to hold the processed training data.\n",
    "- test: A dictionary containing the test data. It has two keys:\n",
    "    - raw: The file path for the test data file.\n",
    "    - loaded: A dictionary to hold the loaded test data.\n",
    "    - processed: A dictionary to hold the processed test data.\n",
    "- meta: The file path for the metadata file.\n",
    "\n",
    ":var data: dict\n",
    "\"\"\"\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor = 'val_loss',\n",
    "        patience = 10,\n",
    "        verbose = 2,\n",
    "        restore_best_weights = True\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor = 'val_loss',\n",
    "        factor = 0.5,\n",
    "        patience = 5,\n",
    "        verbose = 2\n",
    "    )\n",
    "]\n",
    "\"\"\"\n",
    "A list of callbacks for the model training. Currently, it contains\n",
    "the following callbacks:\n",
    "- EarlyStopping: Stops training when a monitored metric has stopped improving.\n",
    "- __ReduceLROnPlateau__: Reduces the learning rate when a metric has stopped improving.\n",
    "\"\"\"\n",
    "\n",
    "configs = {\n",
    "\t'baseModel': VGG16,\n",
    "\t'trainBase': 'block5_conv1',\n",
    "\t'poolingLayer': Flatten,\n",
    "\t'dropoutRate': 0.4,\n",
    "\t'denseUnits': 256,\n",
    "\t'useBatchNorm': True,\n",
    "}\n",
    "\n",
    "regularizers = {\n",
    "\t'none': None,\n",
    "\t'l1': l1(1e-5),\n",
    "\t'l2': l2(1e-4),\n",
    "\t'l1_l2': l1_l2(1e-5, 1e-4)\n",
    "}\n",
    "\"\"\"\n",
    "Defines the regularizers to be used in the model.\n",
    "The dictionary contains the following:\n",
    "- `none`: No regularization.\n",
    "- `l1`: L1 regularization with a strength of 1e-5.\n",
    "- `l2`: L2 regularization with a strength of 1e-4.\n",
    "- `l1_l2`: L1 and L2 regularization with strengths of 1e-5 and 1e-4, respectively.\n",
    "\"\"\"\n",
    "\n",
    "trainDurations = {\n",
    "\t\"none\": 0,\n",
    "\t\"l1\": 0,\n",
    "\t\"l2\": 0,\n",
    "\t\"l1_l2\": 0,\n",
    "}\n",
    "\"\"\"\n",
    "Defines the duration of the entire `fit()` process.\n",
    "\"\"\"\n",
    "\n",
    "batchSize = 24\n",
    "\"\"\"\n",
    "Defines the batch size for the training and validation data.\n",
    "\"\"\"\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a40e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getMetrics(model: Model, dataset: Union[tf.data.Dataset, ImageDataGenerator], logPerBatch = False, yTrue = None) -> tuple:\n",
    "\t\"\"\"\n",
    "\tCalculates the accuracy `(avg)` and accuracy range `(min, max)`\n",
    "\tfor the given model and dataset.\n",
    "\n",
    "\tAlso returns the true labels and predicted labels. The metrics are all floats\n",
    "\tthat represent the score in decimal and not percentage.\n",
    "\n",
    "\tWhen the `yTrue` parameter is not provided, the function will use the dataset to get the\n",
    "\ttrue labels. In doing so, the function will take longer to run as it will have to iterate\n",
    "\tthrough the dataset.\n",
    "\n",
    "\t**NOTE**: Using `logPerBatch` will log the metrics per batch and will slow the process down.\n",
    "\n",
    "\t:param model: The model to use for prediction.\n",
    "\t:type model: tensorflow.keras.models.Model\n",
    "\n",
    "\t:param dataset: The dataset to calculate the metrics for.\n",
    "\t:type dataset: Union[tensorflow.data.Dataset, ImageDataGenerator]\n",
    "\n",
    "\t:param logPerBatch: Whether to log the metrics per batch or not. Default is False.\n",
    "\t:type logPerBatch: bool\n",
    "\n",
    "\t:param yTrue: The true labels of the dataset. Optional.\n",
    "\t:type yTrue: list\n",
    "\n",
    "\t:return: A tuple containing metrics, the true labels, and the predicted labels; wherein the metrics is also a tuple containing the `(avg, min, max)` values.\n",
    "\t:rtype: tuple(tuple, list, list)\n",
    "\t\"\"\"\n",
    "\tmin = 0\n",
    "\tmax = 0\n",
    "\tavg = 0\n",
    "\n",
    "\tskipIteration = True\n",
    "\tif yTrue is None:\n",
    "\t\tskipIteration = False\n",
    "\t\tyTrue = []\n",
    "\tyPred = []\n",
    "\n",
    "\tprint(f\"Calculating metrics for {model.name}...\")\n",
    "\tprint(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "\tif logPerBatch:\n",
    "\t\tprint(\"Predicting...\")\n",
    "\t\tfor x, y in dataset:\n",
    "\t\t\tclasses = model.predict(x, verbose = 0)\n",
    "\t\t\tclasses = np.argmax(classes, axis = 1)\n",
    "\n",
    "\t\t\tyTrue.extend(y)\n",
    "\t\t\tyPred.extend(classes)\n",
    "\n",
    "\t\t\tprint(f\"Classes: {classes}\")\n",
    "\t\t\tprint(f\"Labels: {y}\")\n",
    "\n",
    "\t\t# Accuracy\n",
    "\t\taccuracy = np.sum(classes == y) / len(y)\n",
    "\t\tif accuracy > max:\n",
    "\t\t\tmax = accuracy\n",
    "\t\tif accuracy < min or min == 0:\n",
    "\t\t\tmin = accuracy\n",
    "\t\tavg += accuracy\n",
    "\telse:\n",
    "\t\tprint(\"Predicting...\")\n",
    "\t\tprobs = model.predict(dataset, verbose=0)\n",
    "\t\tprint(f\"Probs: {probs}\")\n",
    "\t\tclasses = np.argmax(probs, axis=1)\n",
    "\n",
    "\t\tif not skipIteration:\n",
    "\t\t\tfor _, y in dataset:\n",
    "\t\t\t\tyTrue.extend(y)\n",
    "\t\tyPred.extend(classes)\n",
    "\n",
    "\t\t# Accuracy\n",
    "\t\taccuracy = np.sum(classes == yTrue) / len(yTrue)\n",
    "\t\tmin = max = avg = accuracy\n",
    "\n",
    "\tprint(f\"Formula: {np.sum(classes == yTrue)} / {len(yTrue)}\")\n",
    "\tprint(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\tprint(f\"Of {len(yTrue)} images, {np.sum(classes == yTrue)} were correct while {np.sum(classes != yTrue)} were incorrect.\")\n",
    "\n",
    "\tavg /= len(dataset)\n",
    "\treturn (avg, min, max), yTrue, yPred\n",
    "\n",
    "def plotModelHistory(modelName, history, accuracy, yTrue, yPred) -> None:\n",
    "\t\"\"\"\n",
    "\tPlots the training history of the model.\n",
    "\n",
    "\t:param modelName: The name of the model.\n",
    "\t:type modelName: str\n",
    "\n",
    "\t:param history: The training history of the model.\n",
    "\t:type history: tensorflow.keras.callbacks.History\n",
    "\n",
    "\t:param accuracy: The accuracy of the model in decimal form (not percentage).\n",
    "\t:type accuracy: float\n",
    "\n",
    "\t:param yTrue: The true labels of the dataset.\n",
    "\t:type yTrue: list\n",
    "\n",
    "\t:param yPred: The predicted labels of the dataset.\n",
    "\t:type yPred: list\n",
    "\t\"\"\"\n",
    "\tstatus = \"Underfitted\" if accuracy < 0.5 else \"Overfitted\" if accuracy > 0.9 else \"Just Right\"\n",
    "\tyTrue = np.sum(np.array(yTrue) == np.array(yPred))\n",
    "\tyLength = len(yPred)\n",
    "\tyScore = yTrue / yLength\n",
    "\n",
    "\taccuracy = accuracy * 100\n",
    "\tunixTime = int(datetime.datetime.now().timestamp() * 1e6)\n",
    "\n",
    "\tprint(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\tprint(f\"Using `forCM`: {yTrue} / {yLength} = {yScore * 100:.2f}%\")\n",
    "\n",
    "\tif not os.path.exists(f\"outputs/accuracy/{modelName}\"):\n",
    "\t\tos.makedirs(f\"outputs/accuracy/{modelName}\")\n",
    "\n",
    "\tplt.figure(figsize = (10, 6))\n",
    "\tplt.plot(history.history['accuracy'], color = 'blue', label = 'train')\n",
    "\tplt.plot(history.history['val_accuracy'], color = 'red', label = 'val')\n",
    "\tplt.legend()\n",
    "\tplt.grid()\n",
    "\tplt.title(f'Accuracy ({modelName})\\nStatus: {status} ({accuracy:.2f}%)')\n",
    "\tplt.xlabel('Epochs')\n",
    "\tplt.ylabel('Accuracy')\n",
    "\tplt.savefig(f\"outputs/accuracy/{modelName}/{unixTime} - {accuracy:.2f}%.png\")\n",
    "\n",
    "def plotConfusionMatrix(yTrue, yPred, modelName, accuracy) -> None:\n",
    "\t\"\"\"\n",
    "\tPlots the confusion matrix for the model predictions.\n",
    "\n",
    "\t:param yTrue: The true labels of the dataset.\n",
    "\t:type yTrue: list\n",
    "\n",
    "\t:param yPred: The predicted labels of the dataset.\n",
    "\t:type yPred: list\n",
    "\n",
    "\t:param modelName: The name of the model.\n",
    "\t:type modelName: str\n",
    "\n",
    "\t:param accuracy: The accuracy of the model in decimal form (not percentage).\n",
    "\t:type accuracy: float\n",
    "\t\"\"\"\n",
    "\taccuracy = accuracy * 100\n",
    "\tunixTime = int(datetime.datetime.now().timestamp() * 1e6)\n",
    "\tconfusion = confusion_matrix(yTrue, yPred)\n",
    "\n",
    "\tplt.figure(figsize = (10, 8))\n",
    "\tsns.heatmap(confusion, annot = True, fmt = 'd', cmap = 'Blues')\n",
    "\n",
    "\tplt.title(f'Confusion Matrix - {modelName}\\nAccuracy: {accuracy:.2f}%')\n",
    "\tplt.xlabel('Predicted Label')\n",
    "\tplt.ylabel('True Label')\n",
    "\n",
    "\tif not os.path.exists(f\"outputs/confusion_matrix/{modelName}\"):\n",
    "\t\tos.makedirs(f\"outputs/confusion_matrix/{modelName}\")\n",
    "\n",
    "\tplt.savefig(f\"outputs/confusion_matrix/{modelName}/{unixTime} - {accuracy:.2f}%.png\")\n",
    "\n",
    "def plotModel(model, accuracy, modelName = None) -> None:\n",
    "\t\"\"\"\n",
    "\tPlots the model architecture.\n",
    "\n",
    "\t:param model: The model to plot.\n",
    "\t:type model: tensorflow.keras.models.Model\n",
    "\n",
    "\t:param accuracy: The accuracy of the model in decimal form (not percentage).\n",
    "\t:type accuracy: float\n",
    "\n",
    "\t:param modelName: The name of the model. Optional.\n",
    "\t:type modelName: str\n",
    "\t\"\"\"\n",
    "\tif modelName is None:\n",
    "\t\tmodelName = model.__name__\n",
    "\n",
    "\tunixTime = int(datetime.datetime.now().timestamp() * 1e6)\n",
    "\tplot_model(model, to_file = f\"outputs/{modelName}_model_{unixTime} - {accuracy}%.png\", show_shapes = True, show_layer_names = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de462e1",
   "metadata": {},
   "source": [
    "### Previews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ffacc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f79528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training data files: {data['train']['raw']}\")\n",
    "print(f\"Testing data file: {data['test']['raw']}\")\n",
    "print(f\"Meta data file: {data['meta']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3ddf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in data['train'][\"raw\"]:\n",
    "    print(f\"Unpickling {file}...\")\n",
    "    batch = unpickle(file)\n",
    "    print(f\"Unpickled {file} with keys: {batch.keys()}\")\n",
    "    print(f\"Batch shape: {batch[b'data'].shape}\")\n",
    "    print(f\"Labels shape: {len(batch[b'labels'])}\")\n",
    "    print(f\"Batch size: {len(batch[b'data'])}\")\n",
    "    print(f\"Batch content size: {len(batch[b'data'][random.randint(0, len(batch[b'data']) - 1)])}\")\n",
    "    print(f\"Peek in the batch: {batch[b'data']}\")\n",
    "    print(f\"Peek in the batch content: {batch[b'data'][random.randint(0, len(batch[b'data']) - 1)]}\")\n",
    "    print(\"==============================================\")\n",
    "    data['train'][\"loaded\"][file] = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252c05c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unpickling {data['test']['raw']}...\")\n",
    "batch = unpickle(data['test']['raw'])\n",
    "print(f\"Unpickled {data['test']['raw']} with keys: {batch.keys()}\")\n",
    "print(f\"Batch shape: {batch[b'data'].shape}\")\n",
    "print(f\"Labels shape: {len(batch[b'labels'])}\")\n",
    "print(f\"Batch size: {len(batch[b'data'])}\")\n",
    "print(f\"Batch content size: {len(batch[b'data'][random.randint(0, len(batch[b'data']) - 1)])}\")\n",
    "print(f\"Peek in the batch: {batch[b'data']}\")\n",
    "print(f\"Peek in the batch content: {batch[b'data'][random.randint(0, len(batch[b'data']) - 1)]}\")\n",
    "print(\"==============================================\")\n",
    "data['test'][\"loaded\"] = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e954e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['meta'] = unpickle(data['meta'])\n",
    "data['meta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d01e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train']['loaded'][f'data\\\\data_batch_{random.randint(1, 5)}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f0f573",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['meta'][b'label_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f541846",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data['train']['loaded'][f'data\\\\data_batch_{random.randint(1, 5)}']\n",
    "targetIndex = random.randint(0, len(target[b'data']) - 1)\n",
    "label = target[b'labels'][targetIndex]\n",
    "label = f\"{data['meta'][b'label_names'][label].decode('UTF-8')} ({label})\"\n",
    "\n",
    "showImg(\n",
    "    target[b'data'][targetIndex].reshape(3, 32, 32).transpose(1, 2, 0),\n",
    "    f\"Label: {label}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e2b492",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['test']['loaded'][b'data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea25ffab",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data['test']['loaded']\n",
    "targetIndex = random.randint(0, len(target[b'data']) - 1)\n",
    "label = target[b'labels'][targetIndex]\n",
    "label = f\"{data['meta'][b'label_names'][label].decode('UTF-8')} ({label})\"\n",
    "\n",
    "showImg(\n",
    "    data['test']['loaded'][b'data'][targetIndex].reshape(3, 32, 32).transpose(1, 2, 0),\n",
    "    f\"Label: {label}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71d9e8c",
   "metadata": {},
   "source": [
    "## Process\n",
    "\n",
    "Here begins the process which includes data splitting and pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f897214",
   "metadata": {},
   "source": [
    "### Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4499f4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train']['processed'] = {\n",
    "    'combined': {\n",
    "        'data': None,\n",
    "        'labels': None,\n",
    "        'generator': None\n",
    "    },\n",
    "    'validation': {\n",
    "        'data': None,\n",
    "        'labels': None,\n",
    "        'generator': None\n",
    "    },\n",
    "}\n",
    "\n",
    "data['test']['processed'] = {\n",
    "    'generator': None\n",
    "}\n",
    "\n",
    "# Splitting the data into training and validation sets\n",
    "x = []\n",
    "y = []\n",
    "for file in data['train']['loaded']:\n",
    "    x.append(data['train']['loaded'][file][b'data'])\n",
    "    y.append(data['train']['loaded'][file][b'labels'])\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    np.concatenate(x),\n",
    "    np.concatenate(y),\n",
    "    test_size = 0.2\n",
    ")\n",
    "\n",
    "# Reshaping the data to match the input shape of the model - Uses 244 since the model is VGG16\n",
    "targetShape = 224\n",
    "targetSize = (targetShape, targetShape)\n",
    "    \n",
    "#######################\n",
    "### VALIDATION DATA ###\n",
    "#######################\n",
    "print(f\"Validation data shape: {x_val.shape}\")\n",
    "data['train']['processed']['validation']['data'] = x_val\n",
    "data['train']['processed']['validation']['labels'] = y_val\n",
    "# data['train']['processed']['validation']['generator'] = makeDataset(\n",
    "# \tdata['train']['processed']['validation']['data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1),\n",
    "# \tdata['train']['processed']['validation']['labels'],\n",
    "# \ttargetSize,\n",
    "# \t# batchSize = batchSize\n",
    "# )\n",
    "\n",
    "data['train']['processed']['validation']['generator'] = (ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    ")).flow(\n",
    "    data['train']['processed']['validation']['data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1),\n",
    "    data['train']['processed']['validation']['labels'],\n",
    "    batch_size = batchSize,\n",
    ")\n",
    "\n",
    "##################\n",
    "### TRAIN DATA ###\n",
    "##################\n",
    "print(f\"Training data shape: {x_train.shape}\")\n",
    "data['train']['processed']['combined']['data'] = x_train\n",
    "data['train']['processed']['combined']['labels'] = y_train\n",
    "# data['train']['processed']['combined']['generator'] = makeDataset(\n",
    "# \tdata['train']['processed']['combined']['data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1),\n",
    "# \tdata['train']['processed']['combined']['labels'],\n",
    "# \ttargetSize,\n",
    "# \tTrue,\n",
    "# \t# batchSize = batchSize\n",
    "# )\n",
    "\n",
    "data['train']['processed']['combined']['generator'] = (ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    rotation_range = 20,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True\n",
    ")).flow(\n",
    "    data['train']['processed']['combined']['data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1),\n",
    "    data['train']['processed']['combined']['labels'],\n",
    "    batch_size = batchSize,\n",
    ")\n",
    "\n",
    "### -------------\n",
    "###\n",
    "### -------------\n",
    "\n",
    "#################\n",
    "### TEST DATA ###\n",
    "#################\n",
    "print(f\"Test data shape: {data['test']['loaded'][b'data'].shape}\")\n",
    "# data['test']['processed']['generator'] = makeDataset(\n",
    "# \tdata['test']['loaded'][b'data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1),\n",
    "# \tdata['test']['loaded'][b'labels'],\n",
    "# \ttargetSize,\n",
    "# \t# batchSize = batchSize\n",
    "# )\n",
    "data['test']['processed']['labels'] = data['test']['loaded'][b'labels']\n",
    "data['test']['processed']['generator'] = (ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    ")).flow(\n",
    "    data['test']['loaded'][b'data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1),\n",
    "    data['test']['loaded'][b'labels'],\n",
    "    batch_size = batchSize,\n",
    ")\n",
    "\n",
    "%run memoryDashboard.py\n",
    "\n",
    "# Free up some memory\n",
    "del data['train']['raw']\n",
    "del data['test']['raw']\n",
    "\n",
    "del data['train']['loaded']\n",
    "del data['test']['loaded']\n",
    "\n",
    "sizes = {\n",
    "\t'spe': len(data['train']['processed']['combined']['data']),\n",
    "\t'vs': len(data['train']['processed']['validation']['data']),\n",
    "}\n",
    "\n",
    "del data['train']['processed']['combined']['data']\n",
    "del data['train']['processed']['validation']['data']\n",
    "\n",
    "nD = data\n",
    "\n",
    "del data\n",
    "data = nD\n",
    "del nD\n",
    "\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "%run memoryDashboard.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd974e86",
   "metadata": {},
   "source": [
    "### Creating the Model\n",
    "\n",
    "Includes the compilation and fitting after using 10 configuration samples to learn how they affect the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3b06d4",
   "metadata": {},
   "source": [
    "#### Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fcfa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\n-------------------')\n",
    "print('-- CONFIGURATION --')\n",
    "print('-------------------\\n\\n')\n",
    "\n",
    "print(f'Base Model: {configs[\"baseModel\"].__name__}')\n",
    "print(f'Pooling Layer: {configs[\"poolingLayer\"].__name__}')\n",
    "print(f'Train Base: {configs[\"trainBase\"]}')\n",
    "print(f'Dropout Rate: {configs[\"dropoutRate\"]}')\n",
    "print(f'Dense Units: {configs[\"denseUnits\"]}')\n",
    "print(f'Use Batch Norm: {configs[\"useBatchNorm\"]}')\n",
    "\n",
    "%run memoryDashboard.py\n",
    "\n",
    "print('\\n\\n--------------------')\n",
    "print('-- BUILDING MODEL --')\n",
    "print('--------------------\\n\\n')\n",
    "\n",
    "# Applies the resizing layer to the input shape; 224 since the model is VGG16\n",
    "inputTensor = Input(shape = (32, 32, 3))\n",
    "resized = Resizing(224, 224)(inputTensor)\n",
    "print(\"Resizing Layer Shape: \", resized.shape)\n",
    "\n",
    "# Creates the base model using the specified base model and input tensor\n",
    "baseModel = VGG16(\n",
    "    weights = 'imagenet',\n",
    "    include_top = False,\n",
    "    input_tensor = resized\n",
    ")\n",
    "\n",
    "baseModel.summary()\n",
    "\n",
    "# Unfreezes the base model layers for training\n",
    "# baseModel.trainable = configs['trainBase'](baseModel, 'block5_conv1')\n",
    "trainableFlag = False\n",
    "for layer in baseModel.layers:\n",
    "\tif layer.name == 'block5_conv1':\n",
    "\t\ttrainableFlag = True\n",
    "\tprint(f\"Layer: {layer.name} - Trainable: {layer.trainable} => {trainableFlag}\")\n",
    "\tlayer.trainable = trainableFlag\n",
    "\n",
    "print(\"\")\n",
    "print(f\"{configs['baseModel'].__name__} Input: {baseModel.input}\")\n",
    "print(f\"{configs['baseModel'].__name__} Output: {baseModel.output}\")\n",
    "\n",
    "# Uncomment if `input_tensor` is not used\n",
    "# baseModel = baseModel(resized)\n",
    "\n",
    "# Applies the pooling layer to the base model output\n",
    "x = Flatten()(baseModel.output)\n",
    "# x = Flatten()(baseModel)\n",
    "print(f\"Pooling Layer Shape: {x.shape}\")\n",
    "\n",
    "# Batch Normalization Layer\n",
    "if configs['useBatchNorm']:\n",
    "\tx = BatchNormalization()(x)\n",
    "\tprint(f\"Batch Normalization Shape: {x.shape}\")\n",
    "\n",
    "# Move `x` to `model`\n",
    "model = x\n",
    "\n",
    "# Change `x` to `dict`\n",
    "x = {\n",
    "\t'none': None,\n",
    "\t'l1': None,\n",
    "\t'l2': None,\n",
    "\t'l1_l2': None,\n",
    "}\n",
    "\n",
    "# Holds the final model to be compiled, trained, and evaluated\n",
    "models = {\n",
    "\t'none': None,\n",
    "\t'l1': None,\n",
    "\t'l2': None,\n",
    "\t'l1_l2': None,\n",
    "}\n",
    "\n",
    "# Adds the batch normalization, dropout, and output layers\n",
    "def addTailLayers(model, useBatchNorm, dropoutRate = 0) -> Model:\n",
    "\t\"\"\"\n",
    "\tAdds the last layers to the model.\n",
    "\n",
    "\t:param model: The model to add the layers to.\n",
    "\t:type model: Model\n",
    "\n",
    "\t:param useBatchNorm: Whether to use batch normalization or not.\n",
    "\t:type useBatchNorm: bool\n",
    "\n",
    "\t:param dropoutRate: The dropout rate to use. Default is 0.\n",
    "\t:type dropoutRate: float\n",
    "\n",
    "\t:return: The model with the added layers.\n",
    "\t:rtype: Model\n",
    "\t\"\"\"\n",
    "\tif useBatchNorm:\n",
    "\t\tmodel = BatchNormalization()(model)\n",
    "\t\tprint(f\"Batch Normalization Shape: {model.shape}\")\n",
    "\n",
    "\tif dropoutRate > 0:\n",
    "\t\tmodel = Dropout(configs['dropoutRate'])(model)\n",
    "\t\tprint(f\"Dropout Layer Shape: {model.shape}\")\n",
    "\treturn model\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44add64e",
   "metadata": {},
   "source": [
    "#### Branching\n",
    "\n",
    "In this section, branching will be tested so that kernel regularizers could be tested. It branches off to several models with each using a different kernel regularizer:\n",
    "\n",
    "- `None`\n",
    "- `l1`\n",
    "- `l2`\n",
    "- `l1_l2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b336ec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for kr in ['none', 'l1', 'l2', 'l1_l2']:\n",
    "\ttxtLen = int((len(kr) + 2) / 2)\n",
    "\ttxtPadding = (len(\"=====================================================\") - txtLen - 2) // 2\n",
    "\ttxt = f\"{'=' * txtPadding} {kr.upper()} {'=' * txtPadding}\"\n",
    "\tprint(txt)\n",
    "\n",
    "\t# Hidden Layer 1 - 256 units\n",
    "\tx[kr] = Dense(configs['denseUnits'], activation = \"relu\", kernel_regularizer = regularizers[kr])(model)\n",
    "\tprint(f\"Hidden Layer Shape: {x[kr].shape}\")\n",
    "\n",
    "\tx[kr] = addTailLayers(x[kr], configs['useBatchNorm'], configs['dropoutRate'])\n",
    "\n",
    "\t# Output Layer - 10 units (for 10 classes)\n",
    "\tx[kr] = Dense(10, activation = \"softmax\", kernel_regularizer = regularizers[kr], dtype = 'float32')(x[kr])\n",
    "\tprint(f\"Output Layer Shape: {x[kr].shape}\")\n",
    "\n",
    "\t# models[kr] = Model(inputs = baseModel.input, outputs = x[kr])\n",
    "\tmodels[kr] = Model(inputs = inputTensor, outputs = x[kr])\n",
    "\tprint(f\"{'=' * len(txt)}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8390254f",
   "metadata": {},
   "source": [
    "#### Compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efb56a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in models:\n",
    "\tprint(f\"Compiling Model Variant: {key.upper()}\")\n",
    "\n",
    "\tmodels[key].compile(\n",
    "\t\toptimizer = Adam(learning_rate = 0.001),\n",
    "\t\tloss = \"sparse_categorical_crossentropy\",\n",
    "\t\tmetrics = [\"accuracy\"]\n",
    "\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7c12e4",
   "metadata": {},
   "source": [
    "#### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68ac59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Batch Size: {batchSize}\")\n",
    "\n",
    "# Steps per Epoch\n",
    "SPE = sizes['spe'] // batchSize\n",
    "\n",
    "# Validation Steps\n",
    "VS = sizes['vs'] // batchSize\n",
    "\n",
    "print(f\"Potential SPE: {SPE}\")\n",
    "print(f\"Potential VS: {VS}\")\n",
    "print(\"\")\n",
    "\n",
    "# SPE = 300\n",
    "# VS = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d986a836",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(models.keys())\n",
    "for key in keys:\n",
    "\tprint(f\"Training Model Regularization Variant: {key.upper()}\\n\")\n",
    "\n",
    "\t%run memoryDashboard.py\n",
    "\tstart = time.time()\n",
    "\thistory = models[key].fit(\n",
    "\t\tdata['train']['processed']['combined']['generator'],\n",
    "\t\tsteps_per_epoch = SPE,\n",
    "\t\tepochs = 100,\n",
    "\t\tvalidation_data = data['train']['processed']['validation']['generator'],\n",
    "\t\tvalidation_steps = VS,\n",
    "\t\tverbose = 2,\n",
    "\t\tcallbacks = callbacks,\n",
    "\t)\n",
    "\tend = time.time()\n",
    "\tprint(\"\")\n",
    "\t%run memoryDashboard.py\n",
    "\n",
    "\ttrainDuration = end - start\n",
    "\n",
    "\ttrainDurations[key] = trainDuration\n",
    "\tprint(f\"Training Duration ({key.upper()}): {trainDuration} seconds / {trainDuration / 60} minutes\\n\")\n",
    "\n",
    "\t# Get the metrics for the model\n",
    "\tmetrics, yTrue, yPred = getMetrics(\n",
    "\t\tmodels[key],\n",
    "\t\tdata['test']['processed']['generator'],\n",
    "\t\tFalse,\n",
    "\t\tyTrue = data['test']['processed']['labels'],\n",
    "\t)\n",
    "\tavg, min, max = metrics\n",
    "\n",
    "\t# Model History\n",
    "\tplotModelHistory(\n",
    "\t\tkey.upper(),\n",
    "\t\thistory,\n",
    "\t\tavg,\n",
    "\t\tyTrue,\n",
    "\t\tyPred\n",
    "\t)\n",
    "\n",
    "\t# Confusion Matrix\n",
    "\tplotConfusionMatrix(\n",
    "\t\tyTrue,\n",
    "\t\tyPred,\n",
    "\t\tkey.upper(),\n",
    "\t\tavg\n",
    "\t)\n",
    "\n",
    "\t# Show model architecture\n",
    "\tplotModel(\n",
    "\t\tmodels[key],\n",
    "\t\tavg,\n",
    "\t\tkey.upper()\n",
    "\t)\n",
    "\n",
    "\t# Free up some memory\n",
    "\tdel history\n",
    "\tdel yTrue\n",
    "\tdel yPred\n",
    "\tdel metrics\n",
    "\tdel avg\n",
    "\tdel min\n",
    "\tdel max\n",
    "\tdel models[key]\n",
    "\n",
    "\tK.clear_session()\n",
    "\tgc.collect()\n",
    "\n",
    "print(f\"\\n\\nTotal Training Duration: {sum(trainDurations.values())} seconds / {sum(trainDurations.values()) / 60} minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
